{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import keras\n",
    "import pickle\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import imageio as io\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Conv2D, average, maximum\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_heatmap(heatmap, center, sigma):\n",
    "    center_x, center_y = center[0],  center[1]\n",
    "    height, width = heatmap.shape\n",
    "\n",
    "    # modify the threshold for our desired dataset\n",
    "    th = 2\n",
    "\n",
    "    delta = math.sqrt(th * 2)\n",
    "\n",
    "    x0 = int(max(0, center_x - delta * sigma))\n",
    "    y0 = int(max(0, center_y - delta * sigma))\n",
    "\n",
    "    x1 = int(min(width, center_x + delta * sigma))\n",
    "    y1 = int(min(height, center_y + delta * sigma))\n",
    "\n",
    "    # gaussian filter\n",
    "    for y in range(y0, y1):\n",
    "        for x in range(x0, x1):\n",
    "            d = (x - center_x) ** 2 + (y - center_y) ** 2\n",
    "            exp = d / 2.0 / sigma / sigma\n",
    "            if exp > th:\n",
    "                continue\n",
    "            heatmap[y][x] = max(heatmap[y][x], math.exp(-exp))\n",
    "            heatmap[y][x] = min(heatmap[y][x], 1.0)\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(label, target_size, scale, sigma):\n",
    "\n",
    "    heatmap_target = []\n",
    "\n",
    "    for coord in range(target_size[2]):\n",
    "        # This is using Gaussian model but in an smart way\n",
    "        heatmap = np.zeros((target_size[0], target_size[1]), dtype=np.float32)\n",
    "        annos = (label[coord] * scale, label[coord + n_keypoints] * scale)\n",
    "        heatmap = put_heatmap(heatmap, annos, sigma)\n",
    "\n",
    "        heatmap_target.append(heatmap)\n",
    "\n",
    "    #  TODO: Using Cauchuy kernel instead of Gaussian distribution\n",
    "\n",
    "    heatmap_target = 10* np.array(heatmap_target).transpose((1, 2, 0))\n",
    "\n",
    "    # background\n",
    "    # heatmap[:, :, -1] = np.clip(1 - np.amax(heatmap, axis=2), 0.0, 1.0)\n",
    "\n",
    "    return heatmap_target.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Ä°zmir Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all images path\n",
    "path_of_images_files_list = glob.glob(\"C:\\\\Users\\\\Mesut\\\\Desktop\\\\5p_3p_Datas\\\\All_Labelling_Outputs_Miko_Mert_Cleared\\\\*\\\\*.jpg\")\n",
    "len(path_of_images_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the backboard images path\n",
    "path_of_bb_images_list = [item for item in path_of_images_files_list if (\"BBOARD\" in item.split(\"\\\\\")[-1] or \"bboard\" in item.split(\"\\\\\")[-1])]\n",
    "len(path_of_bb_images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all txt path\n",
    "path_of_txt_files = glob.glob(\"C:\\\\Users\\\\Mesut\\\\Desktop\\\\5p_3p_Datas\\\\All_Labelling_Outputs_Miko_Mert_Cleared\\\\*\\\\*.txt\")\n",
    "len(path_of_txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the backboard txt path\n",
    "path_of_bb_txt_files_list = [item for item in path_of_txt_files if \"BBOARD_LABELS\" in item.split(\"\\\\\")[-1]]\n",
    "len(path_of_bb_txt_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Keypoints, concat it with images path and images name\n",
    "image_file_name_and_keypoints_and_paths = []\n",
    "for i,item in enumerate(path_of_bb_txt_files_list):\n",
    "    # Pick keypoints\n",
    "    keypoints = open(item, \"r\").read().split(\"\\n\")[1].split(\";\")[1:9]\n",
    "    # pick backboard image path\n",
    "    txt_file_name = item.split(\"\\\\\")[-1]\n",
    "    mutual_name = txt_file_name.split(\".txt\")[0].split(\"BBOARD_LABELS_\")[-1] # Obtain the mutual_name\n",
    "    bb_image_path_list = [element for element in path_of_bb_images_list if mutual_name in element]\n",
    "    bb_image_path = bb_image_path_list[0]\n",
    "    # Pick backboard image file name\n",
    "    image_file_name = bb_image_path.split(\"\\\\\")[-1]\n",
    "    \n",
    "    image_file_name_and_keypoints_and_paths.append([image_file_name, keypoints, bb_image_path, item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our supporter Data Frame\n",
    "columns = [\"Image File Names\", \"Keypoints\", \"Image Paths\", \"Txt Paths\"]\n",
    "df_labels = pd.DataFrame(image_file_name_and_keypoints_and_paths, columns = columns)\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2grey(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the backboad images\n",
    "backboard_images_list = []\n",
    "for path in df_labels[\"Image Paths\"]:\n",
    "    image = io.imread(path)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #image = cv2.imread(path, 0)\n",
    "    grey = rgb2grey(image).reshape(224,224,1)\n",
    "    backboard_images_list.append(grey)\n",
    "backboard_images_list = np.array(backboard_images_list)\n",
    "len(backboard_images_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images list into a numpy array. This is fascinating Man! Powerful syntax!\n",
    "allArrays = np.concatenate([item for item in backboard_images_list])\n",
    "backboards_tensor = allArrays.reshape((len(backboard_images_list), 224,224,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = backboards_tensor\n",
    "y_new = np.vstack(df_labels[\"Keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = backboards_tensor.astype(np.float32)\n",
    "y_new = np.vstack(df_labels[\"Keypoints\"]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(y_new):\n",
    "    y_new[i] = np.hstack((item.reshape((4,2))[:,0], item.reshape((4,2))[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_train_labels(image_tensor, y_labels, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(np.arange(image_tensor.shape[0]), size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = image_tensor[ind:ind+1, :, :, :] # input shape must be (1,256,256,3)!\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        ax.imshow(img[0].reshape(224,224), cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        keypoints = y_labels[ind]\n",
    "        ax.plot(keypoints.reshape((2,4))[0,:],keypoints.reshape((2,4))[1,:], 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_train_labels(X_new, y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_keypoints = 4\n",
    "a = get_heatmap(y_new[0], (224,224,4), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a[:,:,2], cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_new[0].reshape(224,224), cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,:,2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(np.sum(a, axis = -1), cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_masks(image_tensor, y_labels, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(np.arange(image_tensor.shape[0]), size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = image_tensor[ind:ind+1, :, :, :] # input shape must be (1,256,256,3)!\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        keypoints = y_labels[ind]\n",
    "        a = get_heatmap(keypoints, (224,224,4), 1, 5)\n",
    "        summed = np.sum(a, axis = -1)\n",
    "        #summed = img[0].reshape(224,224) + np.sum(a, axis = -1)\n",
    "        max_of_summed = summed.max()\n",
    "        ax.imshow(img[0].reshape(224,224) + summed*(255/max_of_summed), cmap=plt.get_cmap(\"gray\"))\n",
    "        #axes[0,c].imshow(summed, cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        #axes[1,c].imshow(np.sum(a, axis = -1), cmap=plt.get_cmap(\"gray\"))\n",
    "        #ax.plot(keypoints.reshape((2,4))[0,:],keypoints.reshape((2,4))[1,:], 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        #axes[1,c].set_title(str(ind))\n",
    "        ax.axis('off')\n",
    "        #axes[1,c].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new[0].reshape(224,224).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a = np.sum(a, axis = -1)\n",
    "max1 = all_a.max()\n",
    "print(max1)\n",
    "plt.imshow(X_new[0].reshape(224,224) + all_a *(255/max1), cmap=plt.get_cmap(\"gray\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizing_masks(X_new, y_new, nrows=5, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Heatmap Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_keypoints = 4\n",
    "a = get_heatmap(y_new[0], (224,224,4), 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_keypoints = 4\n",
    "heatmap_list = []\n",
    "for item in y_new:\n",
    "    heatmap = get_heatmap(item, (224,224,4), 1, 2)\n",
    "    heatmap_list.append(heatmap)\n",
    "len(heatmap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_heatmap_array = np.vstack(heatmap_list)\n",
    "y_heatmap_tensor = all_heatmap_array.reshape(len(heatmap_list), 224,224,4)\n",
    "y_heatmap_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Train Test Split for Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "test_selection_new = np.random.choice(np.arange(X_new.shape[0]), size=int(X_new.shape[0]*test_size), replace=False)\n",
    "len(test_selection_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_train = np.array([image for index,image in enumerate(X_new) if index not in test_selection_new])\n",
    "X_new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_heatmap_train = np.array([keypoints for index,keypoints in enumerate(y_heatmap_tensor) if index not in test_selection_new])\n",
    "y_heatmap_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the inputs\n",
    "X_new_train = X_new_train / 255.\n",
    "#y_heatmap_train = y_heatmap_train / 224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_train_heatmaps(image_tensor, y_labels, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(np.arange(image_tensor.shape[0]), size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = image_tensor[ind:ind+1, :, :, :] # input shape must be (1,256,256,3)!\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        #ax.imshow(img[0].reshape(224,224), cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        summed = np.sum(y_labels[ind], axis = -1)\n",
    "        max_of_summed = summed.max()\n",
    "        ax.imshow((img[0].reshape(224,224)*255) + (summed*(255/max_of_summed)), cmap=plt.get_cmap(\"gray\"))\n",
    "        \n",
    "        #keypoints = y_labels[ind]\n",
    "        #ax.plot(keypoints.reshape((2,4))[0,:]*224,keypoints.reshape((2,4))[1,:]*224, 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "img = X_new_train[ind:(ind+1),:,:,:]\n",
    "summed = np.sum(y_heatmap_train[ind], axis = -1)\n",
    "max_of_summed = summed.max()\n",
    "plt.imshow((img[0].reshape(224,224)*255) + summed*(255/max_of_summed), cmap=plt.get_cmap(\"gray\"))\n",
    "#plt.imshow(summed*(255/max_of_summed)*224, cmap=plt.get_cmap(\"gray\"))\n",
    "#plt.imshow(img[0].reshape(224,224)*255, cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_train_heatmaps(X_new_train, y_heatmap_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Part(Oh!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras.utils\n",
    "\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Activation, Add, Conv2D, Conv2DTranspose, concatenate, Cropping2D, MaxPooling2D, Reshape, UpSampling2D\n",
    "from keras.models import Input, Model\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNET(input_shape):\n",
    "    def downsample_block(x, block_num, n_filters, pooling_on=True):\n",
    "\n",
    "        x = Conv2D(n_filters, kernel_size=(3, 3), strides=1, padding='same', activation='relu',\n",
    "                   name=\"Block\" + str(block_num) + \"_Conv1\")(x)\n",
    "        x = Conv2D(n_filters, kernel_size=(3, 3), strides=1, padding='same', activation='relu',\n",
    "                   name=\"Block\" + str(block_num) + \"_Conv2\")(x)\n",
    "        skip = x\n",
    "\n",
    "        if pooling_on is True:\n",
    "            x = MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid', name=\"Block\" + str(block_num) + \"_Pool1\")(x)\n",
    "\n",
    "        return x, skip\n",
    "\n",
    "    def upsample_block(x, skip, block_num, n_filters):\n",
    "\n",
    "        x = Conv2DTranspose(n_filters, kernel_size=(2, 2), strides=2, padding='valid', activation='relu',\n",
    "                            name=\"Block\" + str(block_num) + \"_ConvT1\")(x)\n",
    "        x = concatenate([x, skip], axis=-1, name=\"Block\" + str(block_num) + \"_Concat1\")\n",
    "        x = Conv2D(n_filters, kernel_size=(3, 3), strides=1, padding='same', activation='relu',\n",
    "                   name=\"Block\" + str(block_num) + \"_Conv1\")(x)\n",
    "        x = Conv2D(n_filters, kernel_size=(3, 3), strides=1, padding='same', activation='relu',\n",
    "                   name=\"Block\" + str(block_num) + \"_Conv2\")(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    input = Input(input_shape, name=\"Input\")\n",
    "\n",
    "    # downsampling\n",
    "    x, skip1 = downsample_block(input, 1, 64)\n",
    "    x, skip2 = downsample_block(x, 2, 128)\n",
    "    x, skip3 = downsample_block(x, 3, 256)\n",
    "    x, skip4 = downsample_block(x, 4, 512)\n",
    "    x, _ = downsample_block(x, 5, 1024, pooling_on=False)\n",
    "\n",
    "    # upsampling\n",
    "    x = upsample_block(x, skip4, 6, 512)\n",
    "    x = upsample_block(x, skip3, 7, 256)\n",
    "    x = upsample_block(x, skip2, 8, 128)\n",
    "    x = upsample_block(x, skip1, 9, 64)\n",
    "\n",
    "    output = Conv2D(15, kernel_size=(1, 1), strides=1, padding='valid', activation='linear', name=\"output\")(x)\n",
    "    output = Reshape(target_shape=(96*96*15,1))(output)\n",
    "\n",
    "    model = Model(inputs=input, outputs=output, name=\"Output\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "unet = UNET(input_shape=(96, 96, 1))\n",
    "print(unet.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building network here\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "               kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "               kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "\n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    outputs = Conv2D(4, (1, 1), activation='sigmoid')(c9)\n",
    "    output = Reshape(target_shape=(224*224*4, 1))(outputs)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputimg = Input((224,224,1))\n",
    "model = get_unet(inputimg, n_filters = 16, dropout = 0.1, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L2 norm\n",
    "def customLoss_L2(y_true, y_pred):\n",
    "    channel_loss = K.sum(K.square(y_pred - y_true), axis=-1)\n",
    "    total_loss = K.mean(channel_loss, axis=-1)\n",
    "    print(total_loss.shape)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L1 norm\n",
    "def customLoss_L1(y_true, y_pred):\n",
    "    channel_loss = K.sum(K.abs(y_pred - y_true), axis=-1)\n",
    "    total_loss = K.mean(channel_loss, axis=-1)\n",
    "    print(total_loss.shape)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define w1*L2 + w2*L1 norm(regularized)\n",
    "def customLoss_L2L1(y_true, y_pred, w1 = 1, w2 = 10):\n",
    "    channel_loss_l2 = K.sum(K.square(y_pred - y_true), axis=-1)\n",
    "    total_loss_l2 = K.mean(channel_loss_l2, axis=-1)\n",
    "    \n",
    "    channel_loss_l1 = K.sum(K.abs(y_pred - y_true), axis=-1)\n",
    "    total_loss_l1 = K.mean(channel_loss_l1, axis=-1)\n",
    "    \n",
    "    regularized_loss = w1*total_loss_l2 + w2*total_loss_l1\n",
    "    \n",
    "    return regularized_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: L2 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=customLoss_L2, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "history_heatmap = model.fit(X_new_train, y_heatmap_train, \n",
    "                 validation_split=0.08, shuffle=True, \n",
    "                 epochs=epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_predictions(model_heat, X_data, test_select, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(test_select, size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = X_data[ind:ind+1, :, :, :]/255 # input shape must be (1,256,256,3)!\n",
    "        predictions = model_heat.predict(img)*224\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        #ax.imshow(img[0].reshape(224,224), cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        summed = np.sum(predictions, axis = -1)\n",
    "        max_of_summed = summed.max()\n",
    "        ax.imshow((img[0].reshape(224,224)*255) + (summed.reshape((224,224))*(255/max_of_summed)), cmap=plt.get_cmap(\"gray\"))\n",
    "        \n",
    "        #keypoints = y_labels[ind]\n",
    "        #ax.plot(keypoints.reshape((2,4))[0,:]*224,keypoints.reshape((2,4))[1,:]*224, 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.sum(model.predict(X_new[0:0+1, :, :, :]/255)*224, axis = -1).reshape((224,224)), cmap=plt.get_cmap(\"gray\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.predict(X_new[0:0+1, :, :, :]/255)*224, axis = -1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.predict(X_new[0:0+1, :, :, :]/255)*224, axis = -1).reshape((224,224)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.predict(X_new[0:0+1, :, :, :]/255)*224, axis = -1).reshape((224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.sum((model.predict(X_new[0:0+1, :, :, :]/255)*224).reshape((224,224,4)), axis = -1), cmap = plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.predict(X_new[0:0+1, :, :, :]/255), axis = -1).reshape((224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_predictions(model, X_new, test_selection_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_data, test_select, nrows=5, ncols=5):\n",
    "    \"\"\"Plots sampled faces with their truth and predictions.\"\"\"\n",
    "    selection = np.random.choice(test_select, size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(15, 15), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = X_data[ind:ind+1, :, :, :] / 255. # input shape must be (1,256,256,3)!\n",
    "        predictions = model.predict(img)*224\n",
    "        ax.imshow(img[0].reshape(224,224), cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        ax.plot(predictions.reshape((2,4))[0,:],predictions.reshape((2,4))[1,:], 'bo')\n",
    "        ax.set_title(str(ind))\n",
    "        ax.axis('off')\n",
    "    fig.suptitle(\"Model X\", fontsize = 20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: L1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputimg = Input((224,224,1))\n",
    "model_l1 = get_unet(inputimg, n_filters = 16, dropout = 0.1, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1.compile(optimizer=\"adam\", loss=customLoss_L1, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "history_heatmap = model_l1.fit(X_new_train, y_heatmap_train, \n",
    "                 validation_split=0.08, shuffle=True, \n",
    "                 epochs=epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.sum((model_l1.predict(X_new[0:0+1, :, :, :]/255)*224).reshape((224,224,4)), axis = -1), cmap = plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_predictions(model_l1, X_new, test_selection_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: L2 + L1(weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputimg = Input((224,224,1))\n",
    "model_l2l1 = get_unet(inputimg, n_filters = 16, dropout = 0.1, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2l1.compile(optimizer=\"adam\", loss=customLoss_L2L1, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "history_heatmap = model_l2l1.fit(X_new_train, y_heatmap_train, \n",
    "                 validation_split=0.08, shuffle=True, \n",
    "                 epochs=epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_predictions(model_l2l1, X_new, test_selection_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
