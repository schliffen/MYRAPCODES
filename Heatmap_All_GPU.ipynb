{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import keras\n",
    "import pickle\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import imageio as io\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Conv2D, average, maximum\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_heatmap(heatmap, center, sigma):\n",
    "    center_x, center_y = center[0],  center[1]\n",
    "    height, width = heatmap.shape\n",
    "\n",
    "    # modify the threshold for our desired dataset\n",
    "    th = 2\n",
    "\n",
    "    delta = math.sqrt(th * 2)\n",
    "\n",
    "    x0 = int(max(0, center_x - delta * sigma))\n",
    "    y0 = int(max(0, center_y - delta * sigma))\n",
    "\n",
    "    x1 = int(min(width, center_x + delta * sigma))\n",
    "    y1 = int(min(height, center_y + delta * sigma))\n",
    "\n",
    "    # gaussian filter\n",
    "    for y in range(y0, y1):\n",
    "        for x in range(x0, x1):\n",
    "            d = (x - center_x) ** 2 + (y - center_y) ** 2\n",
    "            exp = d / 2.0 / sigma / sigma\n",
    "            if exp > th:\n",
    "                continue\n",
    "            heatmap[y][x] = max(heatmap[y][x], math.exp(-exp))\n",
    "            heatmap[y][x] = min(heatmap[y][x], 1.0)\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(label, target_size, scale, sigma):\n",
    "\n",
    "    heatmap_target = []\n",
    "\n",
    "    for coord in range(target_size[2]):\n",
    "        # This is using Gaussian model but in an smart way\n",
    "        heatmap = np.zeros((target_size[0], target_size[1]), dtype=np.float32)\n",
    "        annos = (label[coord] * scale, label[coord + n_keypoints] * scale)\n",
    "        heatmap = put_heatmap(heatmap, annos, sigma)\n",
    "\n",
    "        heatmap_target.append(heatmap)\n",
    "\n",
    "    #  TODO: Using Cauchuy kernel instead of Gaussian distribution\n",
    "\n",
    "    heatmap_target = 10* np.array(heatmap_target).transpose((1, 2, 0))\n",
    "\n",
    "    # background\n",
    "    # heatmap[:, :, -1] = np.clip(1 - np.amax(heatmap, axis=2), 0.0, 1.0)\n",
    "\n",
    "    return heatmap_target.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Singa Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all images path\n",
    "path_of_singa_images_files_list = glob.glob(\"C:\\\\Users\\\\Mesut\\\\Desktop\\\\5 Points\\\\5p_Jupyter\\\\5p_imgs\\\\*.jpg\")\n",
    "len(path_of_singa_images_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new_labelled = pd.read_json(\"C:\\\\Users\\\\Mesut\\\\Desktop\\\\5 Points\\\\From_Mesut_Murat_5p\\\\new_labelled.json\")\n",
    "df_new_labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_path_of_images = []\n",
    "for item in df_new_labelled[\"file_name\"]:\n",
    "    img_path = [element for element in path_of_singa_images_files_list if item.split(\".\")[0] in element]\n",
    "    picked_path = img_path[0]\n",
    "    ordered_path_of_images.append(picked_path)\n",
    "len(ordered_path_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_images_list = []\n",
    "for path in ordered_path_of_images:\n",
    "    image = cv2.imread(path,0)\n",
    "    corrected_images_list.append(image)\n",
    "corrected_images_list = np.array(corrected_images_list)\n",
    "len(corrected_images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_counter_480 = 0\n",
    "for item in corrected_images_list:\n",
    "    if item.shape == (480,720):\n",
    "        shape_counter_480 += 1\n",
    "print(shape_counter_480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_counter_224 = 0\n",
    "for item in corrected_images_list:\n",
    "    if item.shape == (224,224):\n",
    "        shape_counter_224 += 1\n",
    "print(shape_counter_224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, first 2795 images size are (480,720,3), remainders have (224,224,3) size\n",
    "for i,item in enumerate(corrected_images_list):\n",
    "    if item.shape == (480,720):\n",
    "        corrected_images_list[i] = cv2.resize(item, (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_into_tensor(input_list, n_channels):\n",
    "    all_array = np.vstack(input_list)\n",
    "    to_tensor = all_array.reshape(len(input_list), 224,224, n_channels)\n",
    "    return to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singa_bb_image_tensor = convert_list_into_tensor(corrected_images_list, n_channels = 1)\n",
    "singa_bb_image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_labels(image_tensor, df_label, nrows=5, ncols=5):\n",
    "    \"\"\"Plot Labelling outputs\"\"\"\n",
    "    selection = np.random.choice(np.arange(image_tensor.shape[0]), size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(30, 30), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = image_tensor[ind:ind+1, :, :, :] # input shape must be (1,256,256,3)!\n",
    "        \n",
    "        ax.imshow(img[0].reshape(224,224), cmap=plt.get_cmap(\"gray\")) # for plotting input shape: (256,256,3)\n",
    "        #ax.imshow(img[0]) # for plotting input shape: (256,256,3)\n",
    "        ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizing_labels(singa_bb_image_tensor, df_new_labelled, nrows=5, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Corrupted and Skipped Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_labels = [3051, 3071]\n",
    "wrong_labels = [119, 121, 129, 167, 186, 213, 347, 348, 350, 377, 616, 2303, 2902, 3048]\n",
    "bended_boards = [1805]\n",
    "corrupted_images = [328, 637, 844, 858, 905, 955, 999, 1002, 1011, 1036, 1050, 1177, 1205, 1222, 1225, 1282, 1302, 1349, 1367, 1373, 1387, 1393, 1446, 1480, 1487, 1490, 1500, 1509] + \\\n",
    "[1567, 1573, 1587, 1773, 1875, 1891, 1997, 1998, 2264, 2292, 2361, 2386, 2434, 2476, 2802, 2810, 2811, 2842, 2883, 2897, 2908, 2966, 3002, 3056, 3124, 3128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list = corrupted_labels + wrong_labels + bended_boards + corrupted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_remainder_list = [i for i in range(len(singa_bb_image_tensor)) if i not in del_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_singa_tensor = singa_bb_image_tensor[our_remainder_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_singa_final = df_new_labelled.loc[our_remainder_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizing_labels(final_singa_tensor, df_singa_final,7,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Training Data Singa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_singa_Arrays = np.concatenate([np.array(item).reshape((2,5))[:,:-1] for item in df_singa_final[\"Keypoints\"].values])\n",
    "len(all_Arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_singa_array = all_singa_Arrays.reshape((len(final_singa_tensor),8))\n",
    "final_singa_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_singa = final_singa_tensor.astype(np.float32)\n",
    "y_singa = final_singa_array.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_singa.shape, y_singa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_masks(image_tensor, y_labels, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(np.arange(image_tensor.shape[0]), size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = image_tensor[ind:ind+1, :, :, :] # input shape must be (1,256,256,3)!\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        keypoints = y_labels[ind]\n",
    "        a = get_heatmap(keypoints, (224,224,4), 1, 3)\n",
    "        summed = np.sum(a, axis = -1)\n",
    "        #summed = img[0].reshape(224,224) + np.sum(a, axis = -1)\n",
    "        max_of_summed = summed.max()\n",
    "        ax.imshow(img[0].reshape(224,224) + summed*(255/max_of_summed), cmap=plt.get_cmap(\"gray\"))\n",
    "        #axes[0,c].imshow(summed, cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        #axes[1,c].imshow(np.sum(a, axis = -1), cmap=plt.get_cmap(\"gray\"))\n",
    "        #ax.plot(keypoints.reshape((2,4))[0,:],keypoints.reshape((2,4))[1,:], 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        #axes[1,c].set_title(str(ind))\n",
    "        ax.axis('off')\n",
    "        #axes[1,c].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_keypoints = 4 # Set the number of keypoints into the global variable\n",
    "visualizing_masks(X_singa, y_singa, nrows=5, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Heatmap Tensor Singa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_keypoints = 4 # Set the number of keypoints into the global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap_tensor(y_labels):\n",
    "    heatmap_list = []\n",
    "    for item in y_labels:\n",
    "        heatmap = get_heatmap(item, (224,224,4), 1, 3)\n",
    "        heatmap_list.append(heatmap)\n",
    "    return heatmap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generate_heatmap_tensor(y_singa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_keypoints_singa_list = generate_heatmap_tensor(y_singa)\n",
    "len(heatmap_keypoints_singa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_into_tensor(heatmap_list):\n",
    "    all_heatmap_array = np.vstack(heatmap_list)\n",
    "    y_heatmap_tensor = all_heatmap_array.reshape(len(heatmap_list), 224,224,n_keypoints)\n",
    "    return y_heatmap_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_heatmap_singa_tensor = convert_list_into_tensor(heatmap_keypoints_singa_list)\n",
    "y_heatmap_singa_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_heatmaps(image_tensor, y_labels, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(np.arange(image_tensor.shape[0]), size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = image_tensor[ind:ind+1, :, :, :] # input shape must be (1,256,256,3)!\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        #ax.imshow(img[0].reshape(224,224), cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        summed = np.sum(y_labels[ind], axis = -1)\n",
    "        max_of_summed = summed.max()\n",
    "        ax.imshow((img[0].reshape(224,224)*255) + (summed*(255/max_of_summed)), cmap=plt.get_cmap(\"gray\"))\n",
    "        \n",
    "        #keypoints = y_labels[ind]\n",
    "        #ax.plot(keypoints.reshape((2,4))[0,:]*224,keypoints.reshape((2,4))[1,:]*224, 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizing_heatmaps(X_singa/255., y_heatmap_singa_tensor, nrows=5, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading İzmir Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all images path\n",
    "path_of_izmir_images_files_list = glob.glob(\"C:\\\\Users\\\\Mesut\\\\Desktop\\\\5p_3p_Datas\\\\All_Labelling_Outputs_Miko_Mert_Cleared\\\\*\\\\*.jpg\")\n",
    "len(path_of_images_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the backboard images path\n",
    "path_of_izmir_bb_images_list = [item for item in path_of_izmir_images_files_list if (\"BBOARD\" in item.split(\"\\\\\")[-1] or \"bboard\" in item.split(\"\\\\\")[-1])]\n",
    "len(path_of_izmir_bb_images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all txt path\n",
    "path_of_izmir_txt_files = glob.glob(\"C:\\\\Users\\\\Mesut\\\\Desktop\\\\5p_3p_Datas\\\\All_Labelling_Outputs_Miko_Mert_Cleared\\\\*\\\\*.txt\")\n",
    "len(path_of_izmir_txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the backboard txt path\n",
    "path_of_izmir_bb_txt_files_list = [item for item in path_of_izmir_txt_files if \"BBOARD_LABELS\" in item.split(\"\\\\\")[-1]]\n",
    "len(path_of_izmir_bb_txt_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Keypoints, concat it with images path and images name\n",
    "image_file_name_and_keypoints_and_paths = []\n",
    "for i,item in enumerate(path_of_izmir_bb_txt_files_list):\n",
    "    # Pick keypoints\n",
    "    keypoints = open(item, \"r\").read().split(\"\\n\")[1].split(\";\")[1:9]\n",
    "    # pick backboard image path\n",
    "    txt_file_name = item.split(\"\\\\\")[-1]\n",
    "    mutual_name = txt_file_name.split(\".txt\")[0].split(\"BBOARD_LABELS_\")[-1] # Obtain the mutual_name\n",
    "    bb_image_path_list = [element for element in path_of_izmir_bb_images_list if mutual_name in element]\n",
    "    bb_image_path = bb_image_path_list[0]\n",
    "    # Pick backboard image file name\n",
    "    image_file_name = bb_image_path.split(\"\\\\\")[-1]\n",
    "    \n",
    "    image_file_name_and_keypoints_and_paths.append([image_file_name, keypoints, bb_image_path, item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our supporter Data Frame\n",
    "columns = [\"Image File Names\", \"Keypoints\", \"Image Paths\", \"Txt Paths\"]\n",
    "df_labels = pd.DataFrame(image_file_name_and_keypoints_and_paths, columns = columns)\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2grey(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the backboad images\n",
    "izmir_backboard_images_list = []\n",
    "for path in df_labels[\"Image Paths\"]:\n",
    "    image = io.imread(path)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #image = cv2.imread(path, 0)\n",
    "    grey = rgb2grey(image).reshape(224,224,1)\n",
    "    izmir_backboard_images_list.append(grey)\n",
    "izmir_backboard_images_list = np.array(izmir_backboard_images_list)\n",
    "len(izmir_backboard_images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izmir_backboard_tensor = convert_list_into_tensor(izmir_backboard_images_list, n_channels = 1)\n",
    "izmir_backboard_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_izmir = izmir_backboard_tensor\n",
    "y_izmir = np.vstack(df_labels[\"Keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_izmir = izmir_backboard_tensor.astype(np.float32)\n",
    "y_izmir = np.vstack(df_labels[\"Keypoints\"]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(y_izmir):\n",
    "    y_izmir[i] = np.hstack((item.reshape((4,2))[:,0], item.reshape((4,2))[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_train_labels(image_tensor, y_labels, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(np.arange(image_tensor.shape[0]), size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = image_tensor[ind:ind+1, :, :, :] # input shape must be (1,256,256,3)!\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        ax.imshow(img[0].reshape(224,224), cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        keypoints = y_labels[ind]\n",
    "        ax.plot(keypoints.reshape((2,4))[0,:],keypoints.reshape((2,4))[1,:], 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizing_train_labels(X_izmir, y_izmir, nrows=5, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Heatmap tensor İzmir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_masks(image_tensor, y_labels, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(np.arange(image_tensor.shape[0]), size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = image_tensor[ind:ind+1, :, :, :] # input shape must be (1,256,256,3)!\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        keypoints = y_labels[ind]\n",
    "        a = get_heatmap(keypoints, (224,224,4), 1, 5)\n",
    "        summed = np.sum(a, axis = -1)\n",
    "        #summed = img[0].reshape(224,224) + np.sum(a, axis = -1)\n",
    "        max_of_summed = summed.max()\n",
    "        ax.imshow(img[0].reshape(224,224) + summed*(255/max_of_summed), cmap=plt.get_cmap(\"gray\"))\n",
    "        #axes[0,c].imshow(summed, cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        #axes[1,c].imshow(np.sum(a, axis = -1), cmap=plt.get_cmap(\"gray\"))\n",
    "        #ax.plot(keypoints.reshape((2,4))[0,:],keypoints.reshape((2,4))[1,:], 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        #axes[1,c].set_title(str(ind))\n",
    "        ax.axis('off')\n",
    "        #axes[1,c].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new[0].reshape(224,224).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a = np.sum(a, axis = -1)\n",
    "max1 = all_a.max()\n",
    "print(max1)\n",
    "plt.imshow(X_new[0].reshape(224,224) + all_a *(255/max1), cmap=plt.get_cmap(\"gray\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizing_masks(X_izmir, y_izmir, nrows=5, ncols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Heatmap Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generate_heatmap_tensor(y_izmir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_keypoints_izmir_list = generate_heatmap_tensor(y_izmir)\n",
    "len(heatmap_keypoints_izmir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_heatmap_izmir_tensor = convert_list_into_tensor(heatmap_keypoints_izmir_list, n_channels = n_keypoints)\n",
    "y_heatmap_izmir_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_heatmaps(X_izmir/255., y_heatmap_izmir_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating the Singa and İzmir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.concatenate((X_singa, X_izmir))\n",
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = np.concatenate((y_heatmap_singa_tensor, y_heatmap_izmir_tensor))\n",
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_heatmaps(X_all/255., y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Train Test Split for All Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "test_selection_all = np.random.choice(np.arange(X_all.shape[0]), size=int(X_all.shape[0]*test_size), replace=False)\n",
    "len(test_selection_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_train = np.array([image for index,image in enumerate(X_all) if index not in test_selection_all])\n",
    "X_all_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_train = np.array([keypoints for index,keypoints in enumerate(y_all) if index not in test_selection_all])\n",
    "y_all_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the inputs\n",
    "X_all_train = X_all_train / 255.\n",
    "#y_all_train = y_all_train / 224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizing_heatmaps(X_all_train, y_all_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Part(Oh!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras.utils\n",
    "\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Activation, Add, Conv2D, Conv2DTranspose, concatenate, Cropping2D, MaxPooling2D, Reshape, UpSampling2D\n",
    "from keras.models import Input, Model\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building network here\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "               kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "               kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "\n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    outputs = Conv2D(4, (1, 1), activation='sigmoid')(c9)\n",
    "    output = Reshape(target_shape=(224*224*4, 1))(outputs) # updated this for what? can't remember, owww!\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L2 norm\n",
    "def customLoss_L2(y_true, y_pred):\n",
    "    channel_loss = K.sum(K.square(y_pred - y_true), axis=-1)\n",
    "    total_loss = K.mean(channel_loss, axis=-1)\n",
    "    print(total_loss.shape)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define L1 norm\n",
    "def customLoss_L1(y_true, y_pred):\n",
    "    channel_loss = K.sum(K.abs(y_pred - y_true), axis=-1)\n",
    "    total_loss = K.mean(channel_loss, axis=-1)\n",
    "    print(total_loss.shape)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define w1*L2 + w2*L1 norm(regularized)\n",
    "def customLoss_L2L1(y_true, y_pred, w1 = 32, w2 = 1):\n",
    "    channel_loss_l2 = K.sum(K.square(y_pred - y_true), axis=-1)\n",
    "    total_loss_l2 = K.mean(channel_loss_l2, axis=-1)\n",
    "    \n",
    "    channel_loss_l1 = K.sum(K.abs(y_pred - y_true), axis=-1)\n",
    "    total_loss_l1 = K.mean(channel_loss_l1, axis=-1)\n",
    "    \n",
    "    regularized_loss = w1*total_loss_l2 + w2*total_loss_l1\n",
    "    \n",
    "    return regularized_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: L2 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputimg = Input((224,224,1))\n",
    "model_l2 = get_unet(inputimg, n_filters = 16, dropout = 0.1, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2.compile(optimizer=\"adam\", loss=customLoss_L2, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history_heatmap = model_l2.fit(X_all_train, y_all_train, \n",
    "                 validation_split=0.08, shuffle=True, \n",
    "                 epochs=epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizing_predictions(model_heat, X_data, test_select, nrows=5, ncols=5):\n",
    "    selection = np.random.choice(test_select, size=(nrows*ncols), replace=False)\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=nrows, ncols=ncols)\n",
    "    for ind, ax in zip(selection, axes.ravel()):\n",
    "        img = X_data[ind:ind+1, :, :, :]/255 # input shape must be (1,256,256,3)!\n",
    "        predictions = model_heat.predict(img)*224\n",
    "        #ax.imshow(img[0]) # for plotting input shape must be: (256,256,3)\n",
    "        #ax.imshow(img[0].reshape(224,224), cmap=plt.get_cmap(\"gray\")) # for plotting input shape must be: (256,256,3)\n",
    "        summed = np.sum(predictions, axis = -1)\n",
    "        max_of_summed = summed.max()\n",
    "        ax.imshow((img[0].reshape(224,224)*255) + (summed.reshape((224,224))*(255/max_of_summed)), cmap=plt.get_cmap(\"gray\"))\n",
    "        \n",
    "        #keypoints = y_labels[ind]\n",
    "        #ax.plot(keypoints.reshape((2,4))[0,:]*224,keypoints.reshape((2,4))[1,:]*224, 'ro')\n",
    "        #ax.plot(np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[0,0:4], np.array(df_label.iloc[ind][\"Keypoints\"]).reshape((2,5))[1,0:4], 'ro')\n",
    "        ax.set_title(str(ind))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.sum(model_l2.predict(X_new[0:0+1, :, :, :]/255)*224, axis = -1).reshape((224,224)), cmap=plt.get_cmap(\"gray\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model_l2.predict(X_new[0:0+1, :, :, :]/255)*224, axis = -1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model_l2.predict(X_new[0:0+1, :, :, :]/255)*224, axis = -1).reshape((224,224)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model_l2.predict(X_new[0:0+1, :, :, :]/255)*224, axis = -1).reshape((224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.sum((model_l2.predict(X_new[0:0+1, :, :, :]/255)*224).reshape((224,224,4)), axis = -1), cmap = plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model_l2.predict(X_new[0:0+1, :, :, :]/255), axis = -1).reshape((224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_predictions(model_l2, X_new, test_selection_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: L1 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputimg = Input((224,224,1))\n",
    "model_l1 = get_unet(inputimg, n_filters = 16, dropout = 0.1, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1.compile(optimizer=\"adam\", loss=customLoss_L1, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history_heatmap = model_l1.fit(X_new_train, y_heatmap_train, \n",
    "                 validation_split=0.08, shuffle=True, \n",
    "                 epochs=epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.sum((model_l1.predict(X_new[0:0+1, :, :, :]/255)*224).reshape((224,224,4)), axis = -1), cmap = plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_predictions(model_l1, X_new, test_selection_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: L2 + L1(weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputimg = Input((224,224,1))\n",
    "model_l2l1 = get_unet(inputimg, n_filters = 16, dropout = 0.1, batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2l1.compile(optimizer=\"adam\", loss=customLoss_L2L1, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history_heatmap = model_l2l1.fit(X_new_train, y_heatmap_train, \n",
    "                 validation_split=0.08, shuffle=True, \n",
    "                 epochs=epochs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizing_predictions(model_l2l1, X_new, test_selection_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
